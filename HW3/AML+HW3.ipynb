{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "amazon_data = pd.read_csv(\"/Users/yingzhu/Desktop/sentiment labelled sentences/amazon_cells_labelled.txt\", header=None, \\\n",
    "                    delimiter=\"\\t\",  quoting=3)    \n",
    "\n",
    "yelp_data = pd.read_csv(\"/Users/yingzhu/Desktop/sentiment labelled sentences/yelp_labelled.txt\", header=None, \\\n",
    "                    delimiter=\"\\t\",  quoting=3)  \n",
    "\n",
    "imdb_data = pd.read_csv(\"/Users/yingzhu/Desktop/sentiment labelled sentences/imdb_labelled.txt\", header=None, \\\n",
    "                    delimiter=\"\\t\",  quoting=3)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon data is balanced:  True\n",
      "amazon data's ratio is  1.0\n",
      "yelp data is balanced:  True\n",
      "yelp data's ratio is  1.0\n",
      "imdb data is balanced:  True\n",
      "imdb data's ratio is  1.0\n"
     ]
    }
   ],
   "source": [
    "# Check whether the labels balanced. Report the ratio between two labels\n",
    "amazon_ratio = sum(amazon_data[1]==1)/sum(amazon_data[1]==0)\n",
    "print(\"amazon data is balanced: \",amazon_ratio==1)\n",
    "print(\"amazon data's ratio is \", amazon_ratio)\n",
    "\n",
    "yelp_ratio = sum(yelp_data[1]==1)/sum(yelp_data[1]==0)\n",
    "print(\"yelp data is balanced: \",yelp_ratio==1)\n",
    "print(\"yelp data's ratio is \", yelp_ratio)\n",
    "\n",
    "imdb_ratio = sum(imdb_data[1]==1)/sum(imdb_data[1]==0)\n",
    "print(\"imdb data is balanced: \",imdb_ratio==1)\n",
    "print(\"imdb data's ratio is \", imdb_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add header to Dataframe\n",
    "amazon_data.columns = [\"sentence\",\"score\"]\n",
    "yelp_data.columns = [\"sentence\",\"score\"]\n",
    "imdb_data.columns = [\"sentence\",\"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so there is no way for me to plug it in here in the us unless i go by a converter.\n"
     ]
    }
   ],
   "source": [
    "# Lowercase all the words\n",
    "amazon_data['sentence'] = amazon_data['sentence'].str.lower()\n",
    "yelp_data['sentence'] = yelp_data['sentence'].str.lower()\n",
    "imdb_data['sentence'] = imdb_data['sentence'].str.lower()\n",
    "print(amazon_data['sentence'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/yingzhu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# try some other lemmatization \n",
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "#from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#lmtzr = WordNetLemmatizer()\n",
    "#def lemm(s):\n",
    " #   s_split = s.split()\n",
    "  #  for i in range(len(s_split)):\n",
    "   #     s_split[i] = lmtzr.lemmatize(s_split[i])\n",
    "    #return \" \".join(s_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#amazon_data['sentence'] =amazon_data['sentence'].apply(lemm)\n",
    "#yelp_data['sentence'] =yelp_data['sentence'].apply(lemm)\n",
    "#imdb_data['sentence'] =imdb_data['sentence'].apply(lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#amazon_data['sentence'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Strip punctuation\n",
    "import string\n",
    "def remove_punctuation(s):\n",
    "    s = ''.join([i for i in s if i not in frozenset(string.punctuation)])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    so there is no way for me to plug it in here i...\n",
       "1                            good case excellent value\n",
       "2                                great for the jawbone\n",
       "3    tied to charger for conversations lasting more...\n",
       "4                                     the mic is great\n",
       "5    i have to jiggle the plug to get it to line up...\n",
       "6    if you have several dozen or several hundred c...\n",
       "7              if you are razr owneryou must have this\n",
       "8                    needless to say i wasted my money\n",
       "9                       what a waste of money and time\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_data['sentence'] = amazon_data['sentence'].apply(remove_punctuation)\n",
    "yelp_data['sentence'] = yelp_data['sentence'].apply(remove_punctuation)\n",
    "imdb_data['sentence'] =imdb_data['sentence'].apply(remove_punctuation)\n",
    "amazon_data['sentence'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yingzhu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Strip the stop word\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stripstop(s):\n",
    "    s_split = s.split()\n",
    "    s_split =' '.join([i for i in s_split if i not in cachedStopWords])\n",
    "    return s_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amazon_data['sentence'] = amazon_data['sentence'].apply(stripstop)\n",
    "yelp_data['sentence'] = yelp_data['sentence'].apply(stripstop)\n",
    "imdb_data['sentence'] = imdb_data['sentence'].apply(stripstop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      way plug us unless go converter\n",
       "1                            good case excellent value\n",
       "2                                        great jawbone\n",
       "3    tied charger conversations lasting 45 minutesm...\n",
       "4                                            mic great\n",
       "5         jiggle plug get line right get decent volume\n",
       "6    several dozen several hundred contacts imagine...\n",
       "7                                   razr owneryou must\n",
       "8                            needless say wasted money\n",
       "9                                     waste money time\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_data['sentence'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         way plug us unless go convert\n",
       "1                                  good case excel valu\n",
       "2                                          great jawbon\n",
       "3      tie charger convers last 45 minutesmajor problem\n",
       "4                                             mic great\n",
       "5            jiggl plug get line right get decent volum\n",
       "6     sever dozen sever hundr contact imagin fun sen...\n",
       "7                                      razr ownery must\n",
       "8                               needless say wast money\n",
       "9                                       wast money time\n",
       "10                                  sound qualiti great\n",
       "11             impress go origin batteri extend batteri\n",
       "12    two seper mere 5 ft start notic excess static ...\n",
       "13                                  good qualiti though\n",
       "14                          design odd ear clip comfort\n",
       "15                highli recommend one blue tooth phone\n",
       "16                                   advis everyon fool\n",
       "17                                             far good\n",
       "18                                           work great\n",
       "19    click place way make wonder long mechan would ...\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization of all the words\n",
    "# User Porter Stemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "def lemmP(s):\n",
    "    s_split=s.split()\n",
    "    for j in range(len(s_split)):\n",
    "        s_split[j]= porter_stemmer.stem(s_split[j]) \n",
    "    return \" \".join(s_split)\n",
    "amazon_data['sentence'] = amazon_data['sentence'][0:len(amazon_data['sentence'])].apply(lemmP)\n",
    "yelp_data['sentence'] = yelp_data['sentence'][0:len(yelp_data['sentence'])].apply(lemmP)\n",
    "imdb_data['sentence'] = imdb_data['sentence'][0:len(imdb_data['sentence'])].apply(lemmP)\n",
    "amazon_data['sentence'][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 100\n",
      "400 100\n",
      "400 100\n"
     ]
    }
   ],
   "source": [
    "# Split training and testing set (first 400 label 0 and 400 label 1 as training, the rest as test)\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "amazon_train,amazon_test = train_test_split(amazon_data,train_size =0.8,stratify =amazon_data['score'])\n",
    "yelp_train,yelp_test = train_test_split(yelp_data,train_size =0.8,stratify =yelp_data['score'])\n",
    "imdb_train,imdb_test = train_test_split(imdb_data,train_size =0.8,stratify =imdb_data['score'])\n",
    "print(sum(amazon_train['score']==1),sum(amazon_test['score']==1))\n",
    "print(sum(yelp_train['score']==1),sum(yelp_test['score']==1))\n",
    "print(sum(imdb_train['score']==1),sum(imdb_test['score']==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bag of Word model \n",
    "def getunique(data):\n",
    "    first=[]\n",
    "    for i in data:\n",
    "        second=i.split()\n",
    "        first = list(set(first +second)) # union\n",
    "    return first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary1 = getunique(amazon_train['sentence'])\n",
    "dictionary2 = getunique(yelp_train['sentence'])\n",
    "dictionary3 = getunique(imdb_train['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3691\n"
     ]
    }
   ],
   "source": [
    "total = list(set(dictionary1+dictionary2+dictionary3))\n",
    "print(len(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 2)\n",
      "(600, 2)\n"
     ]
    }
   ],
   "source": [
    "# Count appearance in the dictionary\n",
    "training= [amazon_train, yelp_train, imdb_train]\n",
    "training = pd.concat(training)\n",
    "print(training.shape)\n",
    "\n",
    "test= [amazon_test, yelp_test, imdb_test]\n",
    "test = pd.concat(test)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurevector(length, s, dic):\n",
    "    occur=[]\n",
    "    for i in range(len(s)):\n",
    "        row = [int(0)]*length\n",
    "        s_split = s[i].split()\n",
    "        for j in range(len(s_split)):\n",
    "            if(s_split[j] in dic):\n",
    "                row[total.index(s_split[j])]+=int(1)\n",
    "        occur.append(row)\n",
    "    return occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sentence = list(training['sentence'])\n",
    "train_score = list(training['score'])\n",
    "test_sentence = list(test['sentence'])\n",
    "test_score = list(test['score'])\n",
    "train_occur= featurevector(len(total),train_sentence,total)\n",
    "test_occur = featurevector(len(total),test_sentence,total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3681</th>\n",
       "      <th>3682</th>\n",
       "      <th>3683</th>\n",
       "      <th>3684</th>\n",
       "      <th>3685</th>\n",
       "      <th>3686</th>\n",
       "      <th>3687</th>\n",
       "      <th>3688</th>\n",
       "      <th>3689</th>\n",
       "      <th>3690</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 3691 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...   3681  \\\n",
       "0     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "1     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "\n",
       "   3682  3683  3684  3685  3686  3687  3688  3689  3690  \n",
       "0     0     0     0     0     0     0     0     0     0  \n",
       "1     0     0     0     1     0     0     0     0     0  \n",
       "\n",
       "[2 rows x 3691 columns]"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_occur[100:102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 229, 1017, 1243, 1346, 1774, 1968, 2153, 2213, 2285, 2333, 2753,\n",
      "       2777, 3041, 3280, 3612]),)\n",
      "(array([ 105,  790, 1478, 1683, 1884, 1903, 1932, 2034, 2105, 2355, 2396,\n",
      "       3237, 3391]),)\n"
     ]
    }
   ],
   "source": [
    "print(np.nonzero(train_occur[100]))\n",
    "print(np.nonzero(train_occur[101]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pick your postprocessing strategy\n",
    "#(1) log normalization\n",
    "import math\n",
    "def lognorm(vector):\n",
    "    res=[]\n",
    "    for i in range(len(vector)):\n",
    "        temp =[]\n",
    "        for j in range(len(vector[i])):\n",
    "            temp.append(math.log1p(vector[i][j]+1))\n",
    "        res.append(temp)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(2) l2 normalization\n",
    "def l2norm(vector):\n",
    "    res=[]\n",
    "    for i in range(len(vector)):\n",
    "        deno = np.linalg.norm(vector[i])\n",
    "        if deno==0:\n",
    "            res.append(vector[i])\n",
    "            continue\n",
    "        temp=[]\n",
    "        for j in range(len(vector[i])):\n",
    "            temp.append(vector[i][j]/deno)\n",
    "        res.append(temp)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(3) l1 normalization:\n",
    "def l1norm(vector):\n",
    "    res=[]\n",
    "    for i in range(len(vector)):\n",
    "        deno=np.linalg.norm(vector[i], ord=1)\n",
    "        if deno==0:\n",
    "            res.append(vector[i])\n",
    "            continue\n",
    "        temp=[]\n",
    "        for j in range(len(vector[i])):\n",
    "            temp.append(vector[i][j]/deno)\n",
    "        res.append(temp)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(4) subtracting mean and divide by variance\n",
    "def meansubtract(vector):\n",
    "    res=[]\n",
    "    for i in range(len(vector)):\n",
    "        vec_mean = np.mean(vector[i])\n",
    "        vec_std = np.std(vector[i])\n",
    "        if vec_std==0:\n",
    "            res.append(vector[i])\n",
    "            continue\n",
    "        temp =[]\n",
    "        for j in range(len(vector[i])):\n",
    "            temp.append((vector[i][j]-vec_mean)/vec_std)\n",
    "        res.append(temp)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_occur_log = lognorm(train_occur)\n",
    "train_occur_l1 = l1norm(train_occur)\n",
    "train_occur_l2 = l2norm(train_occur)\n",
    "train_occur_meansub = meansubtract(train_occur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000282557225166\n",
      "6.97856748766e-05\n",
      "0.000272670768\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.var(train_occur_log))\n",
    "print(np.var(train_occur_l1))\n",
    "print(np.var(train_occur_l2))\n",
    "print(np.var(train_occur_meansub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentiment prediction: train a logstic regression model \n",
    "from sklearn.linear_model import LogisticRegression as logR\n",
    "from sklearn.metrics import confusion_matrix as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [600, 2400]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-520-9b45ab8c8c23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_occur_log\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mls_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_occur_log\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mls_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yingzhu/anaconda/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \"\"\"\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yingzhu/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mdiffering_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yingzhu/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yingzhu/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 181\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [600, 2400]"
     ]
    }
   ],
   "source": [
    "test_occur_log = lognorm(train_occur) \n",
    "ls = logR() \n",
    "ls.fit(train_occur_log,train_score) \n",
    "ls_score = ls.score(test_occur_log,test_score) \n",
    "print(ls_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_occur_log = lognorm(train_occur)\n",
    "ls = logR()\n",
    "ls.fit(train_occur_log,train_score)\n",
    "ls_score = ls.score(test_occur_log,test_score)\n",
    "print(ls_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.786666666667\n"
     ]
    }
   ],
   "source": [
    "test_occur_l1 = l1norm(test_occur)\n",
    "ls = logR()\n",
    "ls.fit(train_occur_l1,train_score)\n",
    "ls_score = ls.score(test_occur_l1,test_score)\n",
    "print(ls_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.816666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[255,  45],\n",
       "       [ 65, 235]])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_occur_l2 = l2norm(test_occur)\n",
    "ls = logR()\n",
    "ls.fit(train_occur_l2,train_score)\n",
    "ls_score = ls.score(test_occur_l2,test_score)\n",
    "print(ls_score)\n",
    "# confusion matrix\n",
    "test_pred = ls.predict(test_occur_l2)\n",
    "cm(test_score,test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad\n",
      "great\n",
      "love\n",
      "excel\n",
      "nice\n",
      "best\n",
      "good\n",
      "amaz\n",
      "wonder\n",
      "happi\n"
     ]
    }
   ],
   "source": [
    "# show most siginificant words (showing top 10)\n",
    "logR_coef = ls.coef_[0]\n",
    "rank_coef = np.argsort(logR_coef)\n",
    "for i in range(10):\n",
    "    print(total[rank_coef[-1*i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81\n"
     ]
    }
   ],
   "source": [
    "test_occur_log = lognorm(test_occur)\n",
    "ls = logR()\n",
    "ls.fit(train_occur_log,train_score)\n",
    "ls_score = ls.score(test_occur_log,test_score)\n",
    "print(ls_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.733333333333\n"
     ]
    }
   ],
   "source": [
    "test_occur_meansub = meansubtract(test_occur)\n",
    "ls = logR()\n",
    "ls.fit(train_occur_meansub,train_score)\n",
    "ls_score = ls.score(test_occur_meansub,test_score)\n",
    "print(ls_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81999999999999995"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "from sklearn import cross_validation\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf_B = BernoulliNB()\n",
    "clf_B.fit(train_occur_l2,train_score)\n",
    "clf_Bscore = clf_B.score(test_occur_l2,test_score)\n",
    "clf_Bscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (g)N-gram Model\n",
    "# n gram bagging\n",
    "def getNgram(data, n):\n",
    "    res=[]\n",
    "    for i in range(len(data)):\n",
    "        temp_data = data[i].split()\n",
    "        temp_res=[]\n",
    "        if len(temp_data)<=n:\n",
    "            res.append([data[i]])\n",
    "            continue\n",
    "        for j in range(len(temp_data)-n+1):\n",
    "            temp_res.append(\" \".join(temp_data[j:j+n]))\n",
    "        res.append(temp_res)   \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get featire vector based on n gram\n",
    "def getunique_ngram(data):\n",
    "    first=[]\n",
    "    for i in data:\n",
    "        first = list(set(first + i)) # union\n",
    "    return first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurevector_ngram(length, s, dic):\n",
    "    occur=[]\n",
    "    for i in range(len(s)):\n",
    "        row = [int(0)]*length\n",
    "        for j in range(len(s[i])):\n",
    "            if(s[i][j] in dic):\n",
    "                row[dic.index(s[i][j])]+=int(1)\n",
    "        occur.append(row)\n",
    "    return occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['thank great', 'great servic'], ['pair headphon', 'headphon worst', 'worst ever', 'ever soundwis'], ['good product', 'product good', 'good seller'], ['impress'], ['found product', 'product waaay', 'waaay big']]\n",
      "[['phone work', 'work great'], ['use dirti'], ['great phone'], ['hate earbug', 'earbug avoid', 'avoid phone', 'phone mean'], ['headset easi', 'easi use', 'use everyon', 'everyon love']]\n"
     ]
    }
   ],
   "source": [
    "train_gram2 = getNgram(train_sentence,2)\n",
    "print(train_gram2[:5])\n",
    "test_gram2 = getNgram(test_sentence,2)\n",
    "print(test_gram2[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11336\n"
     ]
    }
   ],
   "source": [
    "total_gram2 = getunique_ngram(train_gram2)\n",
    "print(len(total_gram2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_occur_gram2= featurevector_ngram(len(total_gram2),train_gram2,total_gram2)\n",
    "test_occur_gram2= featurevector_ngram(len(total_gram2),test_gram2,total_gram2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[266,  34],\n",
       "       [167, 133]])"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logistic regression for gram2\n",
    "test_occur_gram2_l2 = l2norm(test_occur_gram2)\n",
    "train_occur_gram2_l2 = l2norm(train_occur_gram2)\n",
    "ls_gram2 = logR()\n",
    "ls_gram2.fit(train_occur_gram2_l2,train_score)\n",
    "ls_gram2_score = ls_gram2.score(test_occur_gram2_l2,test_score)\n",
    "print(ls_gram2_score)\n",
    "# confusion matrix\n",
    "test_pred_gram2 = ls_gram2.predict(test_occur_gram2_l2)\n",
    "cm(test_score,test_pred_gram2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wast time\n",
      "work great\n",
      "highli recommend\n",
      "great phone\n",
      "work fine\n",
      "realli good\n",
      "one best\n",
      "love place\n",
      "good qualiti\n",
      "food good\n"
     ]
    }
   ],
   "source": [
    "# show most siginificant words (showing top 10) for gram2\n",
    "logR_coef_gram2 = ls_gram2.coef_[0]\n",
    "rank_coef_gram2 = np.argsort(logR_coef_gram2)\n",
    "for i in range(10):\n",
    "    print(total_gram2[rank_coef_gram2[-1*i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65666666666666662"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "# Naive Bayes\n",
    "clf_B_gram2 = BernoulliNB()\n",
    "clf_B_gram2.fit(train_occur_gram2_l2,train_score)\n",
    "clf_B_gram2score = clf_B_gram2.score(test_occur_gram2_l2,test_score)\n",
    "clf_B_gram2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (h) PCA for bag of words model\n",
    "def PCA(data,n):\n",
    "    demean_data = data-np.mean(data,axis=0)\n",
    "    cov = np.cov(data,rowvar=False)\n",
    "    e_val,e_vec = np.linalg.eigh(cov)\n",
    "    asc_index = np.argsort(e_val)[::-1]\n",
    "    val= e_val[asc_index]\n",
    "    vec= e_vec[:,asc_index]\n",
    "    e_val = val[:n]\n",
    "    e_vec = vec[:,:n]\n",
    "    return np.dot(data,np.transpose(e_vec.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 10/50/100\n",
    "def log_res(train_data,train_out, test_data,test_out):\n",
    "    ls = logR()\n",
    "    ls.fit(train_data,train_out)\n",
    "    ls_score= ls.score(test_data,test_out)\n",
    "    print(\"Logistic regression accuracy: \", ls_score)\n",
    "    test_pred = ls.predict(test_data)\n",
    "    print(cm(test_score,test_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2400,10) (2400,3662) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-477-cf078ddf9af8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_pac10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_occur_l2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_pac10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_occur_l2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#train_pac50 = PCA(train_occur_l2,50)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#test_pac50 = PCA(test_occur_l2,50)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#train_pac100 = PCA(train_occur_l2,100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-476-297506778800>\u001b[0m in \u001b[0;36mPCA\u001b[0;34m(data, n)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0me_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0me_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmean_add\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2400,10) (2400,3662) "
     ]
    }
   ],
   "source": [
    "train_pac10 = PCA(train_occur_l2,10)\n",
    "test_pac10 = PCA(test_occur_l2,10)\n",
    "#train_pac50 = PCA(train_occur_l2,50)\n",
    "#test_pac50 = PCA(test_occur_l2,50)\n",
    "#train_pac100 = PCA(train_occur_l2,100)\n",
    "#test_pac100 = PCA(test_occur_l2,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy:  0.585\n",
      "[[249  51]\n",
      " [198 102]]\n"
     ]
    }
   ],
   "source": [
    "log_res(train_pac10,train_score,test_pac10,test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy:  0.581666666667\n",
      "[[211  89]\n",
      " [162 138]]\n"
     ]
    }
   ],
   "source": [
    "log_res(train_pac50,train_score,test_pac50,test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy:  0.595\n",
      "[[216  84]\n",
      " [159 141]]\n"
     ]
    }
   ],
   "source": [
    "log_res(train_pac100,train_score,test_pac100,test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
